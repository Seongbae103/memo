# 1. DcGan
> Deep Convolutional Generative Adversarial Networks

# 2. ML / DL
> nn(신경망)의 유무에 따라 : DL / ML 
> ML = DL + 확률
> ML이 DL보다 넓은 개념
- 묵시적
- 샘플데이터를 기반으로 모델 구축
  - (샘플 데이터 = 훈련 데이터)
- ML과 DL 차이점은 통계다 
  - ML은 통계학습

# 3. 데이터 유형
> 이산형(Categorical)
>> 명목(norminal), 순위(ordinal) 
> 
> 연속형(Continuous)
>> 간격(Interval), 비율(Ratio)

## 연역과 귀납
연역은 가정된 전제이다.
귀납은 개인적 경험이다.

## 인코딩(encoding) ↔ 디코딩(decoding) (플로우가 보이는 머신러닝 프로젝트 p.384)
> 문자 → 숫자

### one-hot encoding
> 

## 데이터 분석의 두 가지 접근 방법
1) 확증적 데이터 분석(<a style=color:purple>CDA</a>: Confirmatory Data Analysis) = 추론통계 = (가설 → 특정사례) = 연역
2) 탐색적 데이터 분석(<a style=color:purple>EDA</a>: Exploratory Data Analysis) = 기술통계 = (데이터(특정사례) → 모델) = 귀납 (캐글 데이터셋 페이지에 나온 그래프처럼 표현된게 EDA)
> 인공지능은
>> 지도 : 귀납 → 연역 / 비지도 : 연역 → 연역
> 
>> 지도와 비지도의 차이는 연역과 귀납 중 무엇으로 시작하는지다

세상에 없는걸 만들면 ML, 이미 있는걸 다시 확인하면 통계

### 확률(prob)
- 종류
  - 수학적(선험적) 확률(시작점이 가설) → 사전 : 식으로 존재 → 연역
  - 통계적(경험적) 확률(시작점이 데이터) → 사후 : 식*큰수(data, limit) → 귀납
- 큰수의 법칙 
> 수학적 확률과 통계적 확률이 같아지는 지점이 있다는게 대수법칙
> P(A) = lim(k/n)에서 P(A)는 통계적, 계수lim은 통계적 확률, k/n은 수학
> 로그는 큰 수다 (log2 1024=10에서 10은 log2를 생략하고 1024를 나타내서)
- target(기대값) = 계수 * variable(변수) + constant(상수)
### 통계
> 데이터 + 확률 
### ML을 위한 통계 개념
표본<br/>
우도함수

    > 우도 함수(가능도 함수로 번역되기도 하고, 영어로는 likelihood function 이라 합니다)는 실현된 데이터(혹은 관찰된 데이터 observed data)로 부터 특정 통계 모델의 적합성을 확인하는데 주로 이용됩니다.

대수(큰 수)의 법칙<br/>
베이지안<br/>
분포<br/>
랜덤

---
# 4. Master Algorithm
| Tribe                 | Origins              | Master Algorithm              | 
|-----------------------|----------------------|-------------------------------| 
| Symbolist(기호주의자)      | Logic, Philosophy    | Inverse Deduction(역연역법)       | 
| Connectionists(연결주의자) | Neuroscience(인공지능)   | Backpropagation(역전파)          | 
| Bayesians(통계주의자)      | Statistics           | Probabilistic Inference(확률추론) | 
| Analogizers(분석철학)     | Psychology(심리학)      | Kernal Machines(SVM)          | 
| Evolutionaries        | Evolutionary Biology | Genetic Programming           | 

### 1) Symbolist(기호주의자)

##### (1) 역연역
### 2) Connectionists(연결주의자)
### 3) Bayesians(통계주의자)

- 베이지 정리 → MCMC(bysian을 해야 이해하기 수월하다)
### 4) Analogizers(분석철학)

---
# 5. 학습
> 신경망으로 테크를 탄 경우 지도, 비지도, 강화' 세 가지로 나뉜다

학습은 통계학에서 추정 문제를 해결하는 과정(=추론)이다
- 지도 학습은 샘플을 사용, 비지도는 샘플을 사용 x
- 지도학습은 분류(classification)와 회귀(regress)로 나뉜다
  > 결국 learning =  target을 구하는 modeling이다
  >
  > model은 var를 잡아내서 class(객체)를 시도한다
- 변수는 feature 와 target 으로 나뉜다.
- 상수는 계수와 편향로 나뉜다.
  - 따라서 다음과 같은 식의 구조를 같는다.
  target(예측값, Hat) = 계수 * feature-value + 편향
  특성변수 = 독립변수 = 외생변수 = x변수
  목적변수 = 종속변수 = 내생변수 = y변수
  - 기대값 y는 다시 y와 y Hat(가설)으로 나눠진다
  feature 의 변환은 표준화와 정규화가 있다.
  아웃라이어가 있으면 표준화 나머지는 정규화가 낫다.
  - 계수(係數, coefficient)는 '인자(因子)'라는 뜻으로 쓰인다.
  - 보통 식 앞에 곱해지는 상수를 말한다.
  - 가장 흔한 계수의 개념은 다항식에서 x n 앞에 붙는 수이다.
  학습은 통계학에서  추정문제 해결과정(=추론)이다.
  learning 은 target 을 구하는 modeling 이다.
### 1) 분류
##### 베르누이(홀짝)
- 베르누이 확률 변수 = x(대문자면 여러개)
- 베르누이 시행 : 호출
### 2) 회귀

### 분포
> 분포는 함수다
>> 확률 질량 함수(PMF) : 리턴값이 정수
>
>> 확률 밀도 함수(PDF) : 리턴값이 실수
- dense는 신경망에서 사용한다
> 확률분포는
>> 이산 - 확률질량함수 PMF: 이항분포, 다항분포, 이산균등분포, 푸아송분포, 베르누이분포, (초)기하분포
>
>> 연속 - 확률밀도함수 PDF: 정규분포(=가우스분포), 연속균등분포, 카이제곱분포, 감마분포
''' 강사님 필기 (추후 병합 필요)## 분포
이산확률변수가 따르는 확률분포인 이산확률분포 중,
대표적인 분포인 베르누이분포(Bernoulli Distribution)와 이항분포(Binomial Distribution)에
시행의 결과가 성공이면 1의 값을 갖고, 실패이면 0의 값을 갖는 확률변수 X를
베르누이(Bernoulli) 확률변수라고 하고, 그 분포를 베르누이 분포라고 합니다.
그리고 이렇게 두 가지의 결과만을 갖는 시행을 베르누이 시행이라고 합니다.
성공일 확률은 p, 실패일 확률은 1-p = q가 되겠습니다.
만약 베르누이 시행을 여러 번 하면 어떻게 될까요?
예를 들어 동전 던지기도, 한번만 하고 끝내는 것이 아니라 5번을 던져서 그중 성공(앞면이라고 가정하죠)이 3번 나올 확률을 계산해볼 수도 있지 않을까요?
이때 사용하는 확률분포를 이항분포라고 합니다.
ㅇ 베르누이분포 : X ~ B(1,p)      (1번 만의 베르누이 시행의 성공 확률분포)
ㅇ 이항분포     : X ~ B(n,p)      (n번 베르누이 시행의 성공 확률분포) 
타깃(y)==함수(f(x))==확률(p)
y와 f(x)는 상태가 다르다
확률은 상태를 가진 함수다

'''
### 무상태 프로그래밍 (RESTful)
> 보안은 우리가 판단하지 말고 그냥 다 무상태로 가라
> <a style=color:red>세션 유지는 내부망 외에는 절대 안돼</a>
> <a href="https://www.postman.com/">포스트맨</a>을 사용해 해당 방식을 제대로 적용했는지 확인이 가능하다
<a href="https://okdone.tistory.com/158">참고 링크</a>
- string으로 보내면 상수값으로서 계속 남아있지만 
- 작성은 딕셔너리 구조를 response에 담아서 보낸다

##### 전제
- 객체지향의 구성 요소는 타입과 객체 참조 그리고 행위이다. 속성은 객체지향의 핵심 구성 요소가 아니다.
  - 
- 속성이 상태를 만든다.
- 이상적인 객체 세계는 무상태이다.
- 상태는 객체 세계와 외부 세계간의 소통을 위해서만 발생한다.
  - 객체세계(장고)/외부세계(리액트)/ 소통(네트워크)
속성=상태에서 여러개였던 상태가 함수를 통해 하나의 타깃값을 갖게되면 상태 하나 밖에 없어서 선택할 수 없는 무상태가 된다
속성을 만들지 않는게 좋다 = 
# Terms
### 추론과 예측의 차이는
- 예측(Predict) : 결정론적 알고리즘을 통한 이미 정해진 답을 맞추는 행위
- 추론(Inference) : '대체로 이렇다'라는 미래에 대한 답을 도출하는 행위</br>
 <a style=color:violet>예측은 학습이고 추론은 모델을 돌린 결과</a>
- 역연역


### Ground Truth(모자 벗은 Y)
https://mac-user-guide.tistory.com/14
https://towardsdatascience.com/in-ai-the-objective-is-subjective-4614795d179b
개발자가 원하는(보고 싶은) 답 → 개발자가 줘야한다
정답지인 label과 같을 필요는 없다

### 정규화(Normalization)와 표준화(Standardization = Z-score)
> feature의 변환은 정규화와 표준화가 있다
> 아웃라이어가 있으면 표준화, 나머지는 정규화가 낫다
- 정규화는 이상치 제거x, 표준화는 이상치를 제거한 형태

### 오차, 편차, 잔차
- 편차(Deviation) : 예측값(타깃값,y^)평균에서 예측값이 떨어진 정도(영점 맞춘다고 생각)
- 오차(Error) : 실제값(y)과 예측값(y^)의 차이
- 잔차(Residual) : 실제값과 트레인 단계의 예측값
> 트레인 단계에서는 잔차를 줄이고, 테스트에서는 오차를 줄여야 한다.

### 편향과 편차, 분산
<a href="https://opentutorials.org/module/3653/22071">링크</a><br/>
정답 하나를 맞추기 위해 컴퓨터는 여러 번의 예측값 내놓기를 시도하는데,
컴퓨터가 내놓은 예측값의 동태를 묘사하는 표현이 '편향' 과 '분산'이다

예측값들과 정답이 대체로 떨어져 있으면 결과의 편향(bias)이 높다고 말하고,
예측값들이 자기들끼리 대체로 흩어져있으면 결과의 분산(variance)이 높다고 말합니다.

회귀 문제이든, 분류 문제이든
첫 번째 그림과 같은 상황을 Underfitting = High Bias<br/>
세 번째 그림과 같은 상황(y와 y Hat이 완전 일치)을 Overfitting = High Variance라고 한다
> 오버피팅의 경우는 맞춘게 아니라 답을 알고있었다고 해서 오히려 안좋은 상황이다 95%가 가장 이상적이다

#### 분산과 표준편차
> 표준편차 : 분산을 제곱근으로 줄인 것
##### 추정에 있어 통계학의 손실함수에는 평균제곱오차 또는 음의 로그 우도함수가 있으며
   머신러닝에서도 동일한 손실함수를 사용한다.
##### 우도함수: 우도 함수(가능도 함수로 번역되기도 하고, 영어로는 likelihood function 이라 합니다)
는 실현된 데이터(혹은 관찰된 데이터 observed data)로 부터
특정 통계 모델의 적합성을 확인하는데 주로 이용됩니다.
##### 손실함수 혹은 비용함수(cost function)는 같은 용어로 통계학, 경제학 등에서 널리 쓰이는 함수로
    머신러닝에서도 손실함수는 예측값과 실제값에 대한 오차를 줄이는 데 사용된다.


### MSE vs. CCEE
회귀ML의 손실함수는 MSE
분류ML의 손실함수는 CCEE(Categorical Cross Entropy Error), 활성화 함수로 softmax를 사용한다

### 데이터셋
지금까지 데이터는 train set과 test set 두개로만 나누었지만 train, test 두개로만 분리하는 것은 기초적인 수준, 
보통 현업에서 모델(커스텀모델)을 만들 때는 아래 세 개로 나눈다
- train → 기출
- test(=Unseen data, 학습과 검증이 끝난 모델의 최종 성능 평가용) → 수능
- validation set(학습이 완료된 모델을 검증하기 위한 set(중간점검용)) → 모의고사
  - 수시로 사용해야 되므로 Cross validation을 사용한다


validation dataset is a sample of data held back from training your model that is used to give an estimate of model skill while tuning model’s hyperparameters.
The validation dataset is different from the test dataset that is also held back from the training of the model, but is instead used to give an unbiased estimate of the skill of the final tuned model when comparing or selecting between final models.
There is much confusion in applied machine learning about what a validation dataset is exactly and how it differs from a test dataset.

### criterion 은 표준이다. 동의어로는
standard, normal, norm, average, level 이 있다.

### 엔트로피(영어: entropy, 독일어: entropie):https://ko.wikipedia.org/wiki/%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC
열역학적 계의 유용하지 않은 (일로 변환할 수 없는) 에너지의 흐름을 설명할 때 이용되는 상태 함수다.
통계역학적으로, 주어진 거시적 상태에 대응하는 미시적 상태의 수의 로그로 생각할 수 있다.
엔트로피는 일반적으로 보존되지 않고, 열역학 제2법칙에 따라 시간에 따라 증가한다.
독일의 물리학자 루돌프 클라우지우스가 1850년대 초에 도입하였다.
대개 기호로 라틴 대문자 S를 쓴다.


## 불순도(gini)
### DecisionTree Learning 에서 불순도를 계산하는 3가지 방법
https://m.blog.naver.com/samsjang/220978650404
지니 인덱스
엔트로피
분류오류

### 불순도란 다양한 범주들의 개체들이 얼마나 포함되었는가 정도이다.
여러가지 클래스가 섞여있는 정도이다. 반대로 순수도(purity)는 같은 클래스끼리
얼마나 많이 포함되어 있는지를 말한다.
https://computer-science-student.tistory.com/60



---

## 객체 모델(Object Model)

### criterion
> = 표준

## CCEE(Categorical Cross Entropy Error)

### Entropy
> 사전적인 정의 : 함수 / 통계적 의미 : 기대값

### 지니계수

### Decision tree
random forest 알고리즘의 기본이 되는 알고리즘으로 tree 기반 알고리즘

### 하이퍼파라미터
model : D에서 
d = D(파라미터)  
d.hook() 일 때 hook이 생략된 D의 파라미터를 받는 것

### Random Forest
> 앙상블 학습의 대표


## 가설 (Hypothesis)
t-value (타깃값) : 둘로 나뉜다
p-value (확률값) : 타깃값의 확률
귀무 가설(null hypothesis)이 맞다는 전제 하에,
표본에서 실제로 관측된 통계치와 '같거나 더 극단적인' 통계치가 관측될 확률이다.

> #### 귀무 가설(歸無假說, 영어: null hypothesis, 기호 H0) 또는 영 가설(零假說)
> 통계학에서 처음부터 버릴 것을 예상하는 가설이다.
> 차이가 없거나 의미있는 차이가 없는 경우의 가설이며
> 이것이 맞거나 맞지 않다는 통계학적 증거를 통해 증명하려는 가설이다.
> 예를 들어 범죄 사건에서 용의자가 있을 때
> 형사는 이 용의자가 범죄를 저질렀다는 추정인 대립가설을 세우게 된다.
> 이때 귀무가설은 용의자는 무죄라는 가설이다.
> 통계적인 방법으로 가설검정(hypothesis test)을 시도할 때 쓰인다.
> 로널드 피셔가 1966년에 정의하였다

기술 통계 -(가설)→ 추론통계 = 학습(learning)
가설로부터 추론을 이끌어내는게 머신러닝


## 알고리즘, 함수, 모델 차이
알고리즘(process) : thred(러닝중인 메소드)의 집합 : hook 
로직(if, for) 
함수 : def(밖에 있으면 함수, 안에 있으면 method)
모델 : class

## 함수와 식의 차이
y = ax + b인 식은 즉시 실행돼서 통제가 불가능
f(x)인 함수에서는 필요한 지점에서 사용 가능



## 산술급수/ 기하급수


##
fit_transform()은 train dataset에서만 사용됩니다
transform()은 test data에 적용하기 위해 를 사용한다.

## 행렬 연산(Matrix Operations)

 행렬 표기법 - Matrix Notation

 행렬 덧셈 - Matrix Sum
 스칼라 곱 - Scalar Multiple
 행렬 곱 - Matrix Multiplication
 행렬의 전치 - The transpose of a matrix

## 선형회귀
diff: 로지스틱회귀

---
### 그냥
- ~법은 메소드, ~기능은 함수로 해석해라
- 절차가 있는 메소드는 hook, ML에서의 모델은 클래스(이후에는 파일로도)



---
# 위치정리 필요
## 베르누이 시행(=이진분류)
## 로지스틱 회귀(시그모이드 함수)
<a href = "https://datascienceschool.net/03%20machine%20learning/10.01%20%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html"> 링크 </a>
- 판별함수 : z(표준) = u(평균)인 함수
- 종속변수가 이항분포에 따르고 그 모수(데이터셋)가 독립변수(샘플)에 의존한다
  - 모수는 독립변수(x)들의 집합이니까

---
# ???개념 부족

---

# 코드
12-19
### <a href="https://www.44bits.io/ko/post/django-4-1-release-note-summary">Django 4.1 변경사항 참고</a>
##### axios
- django 4.1부터 프론트에서 axios 사용 x
### <a href="https://velog.io/@joje/8%EC%9E%A5.-%ED%95%A8%EC%88%98-%EA%B8%B0%EB%B0%98-%EB%B7%B0%EC%99%80-%ED%81%B4%EB%9E%98%EC%8A%A4-%EA%B8%B0%EB%B0%98-%EB%B7%B0">링크2</a>
- CBV : 클래스 기반 뷰
- FBV : 함수 기반 뷰

---
# 12-20
## <a href="https://heytech.tistory.com/332">퍼셉트론</a>
- 퍼셉트론은 다수의 값을 입력받아 하나의 값으로 출력하는 알고리즘
- 최초의 분류기
#### 활성화 함수/ 손실함수

## <a href="https://www.letr.ai/blog/story-20211217-2">AI관련 인물</a>
## <a href="http://computing.or.kr/14804/vanishing-gradient-problem%EA%B8%B0%EC%9A%B8%EA%B8%B0-%EC%86%8C%EB%A9%B8-%EB%AC%B8%EC%A0%9C/">그래디언트 베니싱</a>
- 기울기 소실 문제를 해결하기 위한 함수가 Relu

# 12-21
## 합성곱
>  <a style=color:purple>함수(=입력)</a>와 <a style=color:purple>또 다른 함수((=필터=원도우)x커널)</a>를 곱해서 새로운 함수(=feature app)를 리턴하는 연산자
- 함수 f와 함수 g가 있을 때 f*g로 표현한다
- '+, -, *, /, %'와 달리 변수가 아닌 함수 자체를 연산
- <a style=color:purple>이미지 처리에 특화</a>

### 필터
합성곱에서는 행렬구조로 된 뉴런의 집합

### 커널
합성곱에서는 가중치

### 특성맵(=feature map)
합성곱에서 리턴되는 함수

### 패딩
- 합성곱에서 입력 행렬 주위를 가상의 원소(0)으로 채우는 것
- 종류
  - Same Padding : 패딩O
  - Valid Padding : 패딩X (개념적으로만 이해하고 일반적으로는 세임 패딩)

# 12-22
# <a href ="https://www2.deloitte.com/content/dam/Deloitte/kr/Documents/insights/special-focus/ai-and-cognitive-technologies/kr_insights_special-focus-ai-and-cognitive-technologies_01.pdf">인지 기술 (Cognitive technologies)</a>
# <a href="https://bitcodic.tistory.com/94"> 객체 검출(Object Detection)/ 객체 인식(Object Recognition)</a>
> - 순서는 검출 → 인식
> - 유사한 기술이지만

## nn
#### DNN
> 입력층과 출력층 사이에 다수의 은닉층이 있는 형태
- Dense층을 사용
- 레이블 된 데이터 세트가 충분하지 않아도 적용 가능
#### CNN 
> 
- convolution을 사용
- 영상, 음성에서 주로 사용
#### RNN

